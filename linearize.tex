\documentclass{article}[12pt]
\usepackage{amsmath}
\usepackage{hyperref}
\title{Interpreting Neural Nets by Training to Reduce Nonlinearities}
\author{Elod Pal Csirmaz}

\date{\today}
\begin{document}

\maketitle

\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}

Intro

Rule extraction

We investigate feed-forward neural networks
where all nonlinearities are Parametric Rectified Linear Units (PReLUs)
(CITE ?, torch) % https://github.com/torch/nn/blob/master/doc/transfer.md#prelu
% https://arxiv.org/abs/1502.01852
defined as
\[ f(x) = \max(0, x) + a \min(0, x). \]

Let the input of the network be the vector (or tensor, which we flatten) $I=(i_1, i_2, \ldots, i_n)$
and the output be the vector (or tensor) $O=(o_1, o_2, \ldots, o_m)$.

\section{The Neural Network as a Combination of Linear Functions}

It is easy to see that the output $O$ of such a network can be expressed
as a set of linear combinations of $i_x$, each restricted to a particular region of input values
by a set of linear inequalities on $i_x$ (the conditions).

Intuitively, this is true because the output is a linear function of the input
as long as (that is, inside regions of the possible input vectors in which) none of the inputs of PReLUs
cross from negative to positive or \emph{vice versa}; and because the borders of these regions
are linear as well.

More formally, we can show that this statement is true by constructing a suitable
system of linear combinations and inequalities for any given neural network.
For all PReLUs in the network, let us decide whether their inputs are negative, or nonnegative.
If the number of PReLUs is $\delta$, this means $2^\delta$ possible scenarios.
In each scenario, the neural network becomes completely linear, with the output being a linear function of the input,
and, in fact, with all partial results inside the network being linear functions of the input as well.
This entails that the inputs of PReLUs are also linear functions of the input $I$, yielding
the $\delta$ linear inequalities (the conditions) that mark out the region of the input space where this scenario applies.

Naturally, this system of $2^\delta$ linear combinations yielding the output, each controlled by $\delta$
conditions is likely to be redundant, as the conditions may not be independent,
or may be contradictory, in which case the region of the input space will be empty.
But this system will be an accurate description of the output of the neural network nevertheless.

\bigskip

\noindent
While a linear function may seem easy to interpret and to predict for us humans, clearly $2^\delta$
functions will not be so if $\delta$ is in the range of thousands or millions as in most modern neural networks.
We therefore need to construct an approximation of the output of our neural network which is linear in much
larger regions of the input space, and has far less different regions as well.
This is equivalent to trying to find another neural network that approximates our network well,
but contains a small number of PReLUs only.

\section{Reducing PReLUs into Linearities}

We achieve this by continuing training the original neural network while applying a force on each of
its PReLUs that moves its parameter $a$ towards 1.

Note that if $a=1$, then the PReLU degenerates into the identity function $f(x)=x$, and ceases to be a nonlinearity.
In a sense, it disappears, and the neural network ``collapses'' around it, inasmuch as the linear mapping
that precedes the degenerate PReLU and the one after it can now be combined into a single linear map.

This force therefore finds a balance between approximating the training data
and reducing the number of PReLUs, thereby yielding a neural network that if feasible to express
as a set of linear functions and inequalities for human consumption.
By removing nonlinearities, this force also reduces the capacity of the neural network,
and therefore we expect that it can also be helpful to avoid overfitting.

This force is a static force applied after each backpropagation step, and moves the parameter by an adjustment rate
$\eta_p$ but never above 1:
\[ a_{t+1} = \max\left(0,\;\min\big(1,\;a_t + \eta_p\,\mathrm{sgn}(1-a_t)\,\big)\right) \]

In our research we considered any PReLU fully linear if
\[ a > 0.9995 \]

We also made $\eta_p$ dependent on the current error exhibited by the network,
to allow the network initially to train without interference, and then
pull the PReLU parameters to 1 more and more aggressively. We chose
\[ \eta_p = \eta_0\,0.01\,\max\big(0,\; -\log_{10}(err)-2\big) \]
where $\eta_0=0.01$ is the learning rate for the whole model, and $err$ is the training error.
This means that $\eta_p$ will start becoming non-zero when the training error
falls below 0.01, and can grow indefinitely as the training error falls
(although the $a$ parameters are clipped anyway).

\section{Example}

A toy example network demonstrating the above is available at
\url{https://github.com/csirmaz/trained-linearization}.
It is implemented in Lua/Torch, has 4 input nodes, 3 output nodes, and
4 hidden layers in between with 5 nodes on each.
These layers are linked by trainable fully-connected layers
with PReLU activation following them, except for the last fully connected layer,
which simply provides the output.

The neural network is expected to learn the following correspondences:
\begin{align}
\mathrm{out}_1 &= \mathrm{in}_1 \,\mathrm{xor}\, \mathrm{in}2 \\
\mathrm{out}_1 &= \mathrm{in}_1 \,\mathrm{xor}\, \mathrm{in}2 \\
\mathrm{out}_3 &= \mathrm{in}_1 \cdot \mathrm{in}
\end{align}
Where all the inputs and outputs are 0 or 1, with some extra noise applied to the inputs.
We generate the training data accordingly, based on the $2^4$ possible combinations of 0 and 1 for the input.


\begin{verbatim}
IF +0.00*in1 +0.00*in2 -1.00*in3 -1.00*in4 +1.00 < 0  (PReLU #3 on level 1 is neg. ln(1-weight)=-2.58)  
IF +1.00*in1 +1.00*in2 -0.00*in3 -0.00*in4 -1.00 < 0  (PReLU #1 on level 4 is neg. ln(1-weight)=-2.60)  
THEN    
  out1 = +1.01*in1 +1.01*in2 -0.00*in3 -0.00*in4 +0.00  
  out2 = -0.00*in1 -0.00*in2 -1.03*in3 -1.03*in4 +2.04  
  out3 = -0.00*in1 -0.00*in2 +0.00*in3 +0.00*in4 -0.00  
        
IF +0.00*in1 +0.00*in2 -1.00*in3 -1.00*in4 +1.00 < 0  (PReLU #3 on level 1 is neg. ln(1-weight)=-2.58)  
IF +1.00*in1 +1.00*in2 -0.00*in3 -0.00*in4 -1.00 > 0  (PReLU #1 on level 4 is pos. ln(1-weight)=-2.60)  
THEN    
  out1 = -1.01*in1 -1.01*in2 +0.00*in3 +0.00*in4 +2.02  
  out2 = -0.00*in1 -0.00*in2 -1.03*in3 -1.03*in4 +2.04  
  out3 = +1.00*in1 +1.00*in2 +0.00*in3 +0.00*in4 -1.01  
        
IF +0.00*in1 +0.00*in2 -1.00*in3 -1.00*in4 +1.00 > 0  (PReLU #3 on level 1 is pos. ln(1-weight)=-2.58)  
IF +1.00*in1 +1.00*in2 +0.00*in3 +0.00*in4 -1.00 < 0  (PReLU #1 on level 4 is neg. ln(1-weight)=-2.60)  
THEN    
  out1 = +1.01*in1 +1.01*in2 +0.00*in3 +0.00*in4 +0.00  
  out2 = -0.00*in1 -0.00*in2 +1.00*in3 +1.00*in4 +0.01  
  out3 = -0.00*in1 -0.00*in2 -0.00*in3 -0.00*in4 +0.00  
        
IF +0.00*in1 +0.00*in2 -1.00*in3 -1.00*in4 +1.00 > 0  (PReLU #3 on level 1 is pos. ln(1-weight)=-2.58)  
IF +1.00*in1 +1.00*in2 +0.00*in3 +0.00*in4 -1.00 > 0  (PReLU #1 on level 4 is pos. ln(1-weight)=-2.60)  
THEN    
  out1 = -1.01*in1 -1.01*in2 -0.00*in3 -0.00*in4 +2.02  
  out2 = -0.00*in1 -0.00*in2 +1.00*in3 +1.00*in4 +0.01  
  out3 = +1.00*in1 +1.00*in2 -0.00*in3 -0.00*in4 -1.01
\end{verbatim}



-- TODO If not completely linear, approximate using average slope


\end{document}
